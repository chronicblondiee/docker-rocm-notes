version: '3.8'

services:
  llama-server:
    image: rocm/llama.cpp:llama.cpp-b5997_rocm6.4.0_ubuntu24.04_server
    container_name: llama-server

    # GPU device access for AMD ROCm
    devices:
      - /dev/kfd
      - /dev/dri

    # Security and performance options
    security_opt:
      - seccomp=unconfined

    # Add video group for GPU access
    group_add:
      - video

    # Enable shared memory for multi-GPU/multi-process
    ipc: host

    # Increase shared memory for large models
    shm_size: 16g

    # Volume mount for models (create ./models directory on host)
    volumes:
      - ${MODELS_DIR:-./models}:/data:ro

    # Expose llama.cpp server port
    ports:
      - "${LLAMA_PORT:-8000}:8000"

    # Environment variables
    environment:
      - HIP_VISIBLE_DEVICES=${GPU_ID:-0}

    # Server startup command
    # Set MODEL_PATH to the full path to your .gguf file inside the container
    command:
      - "-m"
      - "${MODEL_PATH:-/data/model.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "-ngl"
      - "${GPU_LAYERS:-99}"
      - "-c"
      - "${CONTEXT_SIZE:-4096}"

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
