version: '3.8'

services:
  llama-server:
    image: rocm/llama.cpp:llama.cpp-b5997_rocm6.4.0_ubuntu24.04_server
    container_name: llama-server

    # GPU device access for AMD ROCm
    devices:
      - /dev/kfd
      - /dev/dri

    # Security and performance options
    security_opt:
      - seccomp=unconfined

    # Add video group for GPU access
    group_add:
      - video

    # Enable shared memory for multi-GPU/multi-process
    ipc: host

    # Increase shared memory for large models
    shm_size: 16g

    # Volume mount for models (create ./models directory on host)
    volumes:
      - ./models:/data:ro

    # Expose llama.cpp server port
    ports:
      - "8000:8000"

    # Environment variables
    environment:
      - HIP_VISIBLE_DEVICES=0

    # Server startup command
    # Replace /data/model.gguf with your actual model filename
    command:
      - "-m"
      - "/data/model.gguf"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "-ngl"
      - "99"
      - "-c"
      - "4096"

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
