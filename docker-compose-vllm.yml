# Docker Compose for vLLM with ROCm
# Backend: vLLM
# Use Case: Production - Highest performance, OpenAI-compatible API
# Usage: docker-compose -f docker-compose-vllm.yml up -d

version: '3.8'

services:
  vllm-server:
    image: rocm/vllm:rocm6.4.1_vllm_0.10.1_20250909
    container_name: vllm-production

    # GPU device access - Manual device mounting
    devices:
      - /dev/kfd
      - /dev/dri

    # Security and performance options (required for HPC workloads)
    security_opt:
      - seccomp=unconfined

    # Add video group for GPU access
    group_add:
      - video

    # Enable shared memory for multi-GPU/multi-process communication
    ipc: host

    # Increase shared memory for large models (16GB minimum, adjust as needed)
    shm_size: 16g

    # Optional: Set memory limits (adjust based on your system)
    # mem_limit: 32g
    # memswap_limit: 32g

    # Volume mount for HuggingFace format models
    # Mount as read-only for security
    volumes:
      - ${MODELS_DIR:-./models}:/models:ro
      # Optional: Cache directory for HuggingFace downloads
      - ${HF_CACHE_DIR:-./hf-cache}:/root/.cache/huggingface

    # Expose vLLM server API port (OpenAI-compatible)
    ports:
      - "${VLLM_PORT:-8000}:8000"

    # Environment variables
    environment:
      - HIP_VISIBLE_DEVICES=${GPU_ID:-0}        # GPU selection (0 = first GPU, 0,1 = multi-GPU)
      - PYTORCH_ROCM_ARCH=${ROCM_ARCH:-gfx1100} # Set to your GPU architecture
      # Optional: HuggingFace token for gated models
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}

    # vLLM server startup command
    # Set MODEL_NAME to your model path inside container or HuggingFace model ID
    # Examples: /models/llama-2-7b-hf or meta-llama/Llama-2-7b-chat-hf
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model ${MODEL_NAME:-/models/llama-2-7b-hf}
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size ${TENSOR_PARALLEL:-1}
      --max-num-seqs ${MAX_SEQS:-256}
      --max-model-len ${MAX_CONTEXT:-4096}
      --gpu-memory-utilization ${GPU_MEM_UTIL:-0.9}
      --enforce-eager
      --disable-log-requests

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# Environment Variables:
# All variables have defaults - override by setting them before running docker-compose
#
# MODELS_DIR      - Host directory containing your models (default: ./models)
# HF_CACHE_DIR    - HuggingFace cache directory (default: ./hf-cache)
# MODEL_NAME      - Model path in container or HF model ID (default: /models/llama-2-7b-hf)
# VLLM_PORT       - Host port to expose (default: 8000)
# GPU_ID          - GPU device ID (default: 0, use "0,1" for multi-GPU)
# ROCM_ARCH       - GPU architecture (default: gfx1100)
# HF_TOKEN        - HuggingFace API token for gated models (default: empty)
# TENSOR_PARALLEL - Number of GPUs for tensor parallelism (default: 1)
# MAX_SEQS        - Max concurrent sequences (default: 256)
# MAX_CONTEXT     - Max context length (default: 4096)
# GPU_MEM_UTIL    - GPU memory utilization 0.0-1.0 (default: 0.9)
#
# GPU Architecture values for ROCM_ARCH:
# - gfx906   : AMD Instinct MI50
# - gfx908   : AMD Instinct MI100
# - gfx90a   : AMD Instinct MI210/MI250/MI250X
# - gfx942   : AMD Instinct MI300X/MI300A
# - gfx1030  : Radeon RX 6900/6800 XT
# - gfx1100  : Radeon RX 7900 XTX/XT
# - gfx1101  : Radeon RX 7800/7700 XT
# - gfx1102  : Radeon RX 7600
#
# Setup instructions:
#
# Method 1: Using environment variables (recommended)
# 1. Set environment variables:
#    export MODELS_DIR="$HOME/models"
#    export MODEL_NAME="/models/llama-2-7b-chat-hf"
#    export ROCM_ARCH="gfx1100"
#    export HF_TOKEN="your_huggingface_token"
#
# 2. Start the service:
#    docker-compose -f docker-compose-vllm.yml up -d
#
# Method 2: Using .env file
# 1. Create a .env file:
#    cat > .env << EOF
#    MODELS_DIR=/home/user/models
#    HF_CACHE_DIR=/home/user/.cache/huggingface
#    MODEL_NAME=/models/llama-2-7b-chat-hf
#    VLLM_PORT=8000
#    GPU_ID=0
#    ROCM_ARCH=gfx1100
#    HF_TOKEN=your_huggingface_token
#    TENSOR_PARALLEL=1
#    MAX_SEQS=256
#    MAX_CONTEXT=4096
#    GPU_MEM_UTIL=0.9
#    EOF
#
# 2. Start the service:
#    docker-compose -f docker-compose-vllm.yml up -d
#
# Method 3: Download model using HuggingFace ID
# 1. Set HF token and model ID:
#    export HF_TOKEN="your_token"
#    export MODEL_NAME="meta-llama/Llama-2-7b-chat-hf"
#    export ROCM_ARCH="gfx1100"
#
# 2. Start (vLLM will auto-download):
#    docker-compose -f docker-compose-vllm.yml up -d
#
# Method 4: Default setup with local models
# 1. Create directories and download model:
#    mkdir -p models hf-cache
#    git lfs install
#    git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf models/llama-2-7b-hf
#
# 2. Start the service:
#    docker-compose -f docker-compose-vllm.yml up -d
#
# Common Commands:
# - View logs:
#   docker-compose -f docker-compose-vllm.yml logs -f
#
# - Test the API (OpenAI-compatible):
#   curl http://localhost:8000/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model": "your-model-name", "messages": [{"role": "user", "content": "Hello!"}]}'
#
# - Stop the service:
#   docker-compose -f docker-compose-vllm.yml down
#
# Multi-GPU setup:
# - Set GPU_ID=0,1 for 2 GPUs
# - Set TENSOR_PARALLEL=2 to match number of GPUs
# - Ensure sufficient system RAM and VRAM
