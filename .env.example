# Docker Compose Environment Variables
# Copy this file to .env and customize for your setup
# Usage: cp .env.example .env

# =============================================================================
# llama.cpp Configuration (docker-compose-llamacpp.yml)
# =============================================================================

# Host directory containing GGUF models
MODELS_DIR=./models

# Full path to .gguf file inside container (relative to /data mount point)
# Example: /data/llama-2-7b-chat.Q4_K_M.gguf
MODEL_PATH=/data/model.gguf

# Host port for llama.cpp server
LLAMA_PORT=8000

# GPU device ID (0 for first GPU, 1 for second, etc.)
GPU_ID=0

# Number of layers to offload to GPU (99 = all layers)
GPU_LAYERS=99

# Context window size (tokens)
CONTEXT_SIZE=4096

# Number of parallel request slots (1 = sequential, 2-4 = concurrent)
# Higher values allow more concurrent requests but use more VRAM
PARALLEL_SLOTS=1

# =============================================================================
# OpenAI API Compatibility (llama.cpp)
# =============================================================================
# The --jinja flag is ENABLED BY DEFAULT in docker-compose-llamacpp.yml
# This enables OpenAI-style tool/function calling support.
#
# Supported OpenAI endpoints:
# - POST /v1/chat/completions  (with tool calling)
# - POST /v1/completions
# - POST /v1/embeddings
# - GET  /v1/models
#
# Tool calling compatible models (use with MODEL_PATH above):
# - Llama 3.1 / 3.3 (8B, 70B) - Best overall choice for tool calling
# - Mistral Nemo (12B)
# - Qwen 2.5 (7B, 14B, 32B)
# - Hermes 2 / 3
# - Functionary v3.1 / v3.2
# - Firefunction v2
#
# Important notes:
# - Granite models do NOT support tool calling
# - Use Q4_K_M or higher quantization (extreme quantization degrades tools)
# - See OPENAI_API_GUIDE.md for detailed usage examples

# =============================================================================
# vLLM Configuration (docker-compose-vllm.yml)
# =============================================================================

# Host directory containing HuggingFace models
# MODELS_DIR=./models

# HuggingFace cache directory
HF_CACHE_DIR=./hf-cache

# Model name: either path in container or HuggingFace model ID
# Examples: /models/llama-2-7b-hf or meta-llama/Llama-2-7b-chat-hf
MODEL_NAME=/models/llama-2-7b-hf

# Host port for vLLM server
VLLM_PORT=8000

# GPU architecture - see list below
ROCM_ARCH=gfx1100

# HuggingFace API token (required for gated models like LLaMA)
HF_TOKEN=

# Number of GPUs for tensor parallelism
TENSOR_PARALLEL=1

# Maximum concurrent sequences
MAX_SEQS=256

# Maximum context length
MAX_CONTEXT=4096

# GPU memory utilization (0.0-1.0)
GPU_MEM_UTIL=0.9

# =============================================================================
# Ollama Configuration (docker-compose-ollama.yml)
# =============================================================================
# Ollama has built-in OpenAI API compatibility - no special configuration needed
# Tool calling works automatically with compatible models
#
# Supported OpenAI endpoints (at http://localhost:11434):
# - POST /v1/chat/completions  (with tool calling support)
# - POST /v1/completions
# - POST /v1/embeddings
# - GET  /v1/models
#
# Ollama also provides native API (easier to use):
# - POST /api/chat
# - POST /api/generate
# - POST /api/embeddings
#
# Tool calling compatible models (via ollama pull):
# - llama3.1:8b or llama3.3:70b - Best choice
# - mistral-nemo - Excellent performance
# - qwen2.5:7b - Good multilingual support

# Docker volume or host path for Ollama data
# Use named volume: OLLAMA_DATA=ollama-data
# Or host path: OLLAMA_DATA=/home/user/.ollama
OLLAMA_DATA=ollama-data

# Host port for Ollama API
OLLAMA_PORT=11434

# GPU_ID already defined above

# =============================================================================
# GPU Architecture Reference (for ROCM_ARCH)
# =============================================================================
# gfx906   - AMD Instinct MI50
# gfx908   - AMD Instinct MI100
# gfx90a   - AMD Instinct MI210/MI250/MI250X
# gfx942   - AMD Instinct MI300X/MI300A
# gfx1030  - Radeon RX 6900/6800 XT
# gfx1100  - Radeon RX 7900 XTX/XT
# gfx1101  - Radeon RX 7800/7700 XT
# gfx1102  - Radeon RX 7600

# =============================================================================
# Example Configurations
# =============================================================================

# Example 1: Using LM Studio models with llama.cpp
# MODELS_DIR=/home/user/.lmstudio/models
# MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
# LLAMA_PORT=8000
# GPU_ID=0

# Example 2: Using vLLM with local HuggingFace model
# MODELS_DIR=/home/user/hf-models
# MODEL_NAME=/models/llama-2-7b-chat-hf
# VLLM_PORT=8000
# ROCM_ARCH=gfx1100
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx

# Example 3: Using vLLM with HuggingFace auto-download
# MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
# ROCM_ARCH=gfx1100

# Example 4: Multi-GPU setup
# GPU_ID=0,1
# TENSOR_PARALLEL=2
