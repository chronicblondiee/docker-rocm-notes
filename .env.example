# Docker Compose Environment Variables
# Copy this file to .env and customize for your setup
# Usage: cp .env.example .env

# =============================================================================
# llama.cpp Configuration (docker-compose-llama.yml)
# =============================================================================

# Host directory containing GGUF models
MODELS_DIR=./models

# Full path to .gguf file inside container (relative to /data mount point)
# Example: /data/llama-2-7b-chat.Q4_K_M.gguf
MODEL_PATH=/data/model.gguf

# Host port for llama.cpp server
LLAMA_PORT=8000

# GPU device ID (0 for first GPU, 1 for second, etc.)
GPU_ID=0

# Number of layers to offload to GPU (99 = all layers)
GPU_LAYERS=99

# Context window size (tokens)
CONTEXT_SIZE=4096

# =============================================================================
# vLLM Configuration (docker-compose-production.yml)
# =============================================================================

# Host directory containing HuggingFace models
# MODELS_DIR=./models

# HuggingFace cache directory
HF_CACHE_DIR=./hf-cache

# Model name: either path in container or HuggingFace model ID
# Examples: /models/llama-2-7b-hf or meta-llama/Llama-2-7b-chat-hf
MODEL_NAME=/models/llama-2-7b-hf

# Host port for vLLM server
VLLM_PORT=8000

# GPU architecture - see list below
ROCM_ARCH=gfx1100

# HuggingFace API token (required for gated models like LLaMA)
HF_TOKEN=

# Number of GPUs for tensor parallelism
TENSOR_PARALLEL=1

# Maximum concurrent sequences
MAX_SEQS=256

# Maximum context length
MAX_CONTEXT=4096

# GPU memory utilization (0.0-1.0)
GPU_MEM_UTIL=0.9

# =============================================================================
# Ollama Configuration (docker-compose-development.yml)
# =============================================================================

# Docker volume or host path for Ollama data
# Use named volume: OLLAMA_DATA=ollama-data
# Or host path: OLLAMA_DATA=/home/user/.ollama
OLLAMA_DATA=ollama-data

# Host port for Ollama API
OLLAMA_PORT=11434

# GPU_ID already defined above

# =============================================================================
# GPU Architecture Reference (for ROCM_ARCH)
# =============================================================================
# gfx906   - AMD Instinct MI50
# gfx908   - AMD Instinct MI100
# gfx90a   - AMD Instinct MI210/MI250/MI250X
# gfx942   - AMD Instinct MI300X/MI300A
# gfx1030  - Radeon RX 6900/6800 XT
# gfx1100  - Radeon RX 7900 XTX/XT
# gfx1101  - Radeon RX 7800/7700 XT
# gfx1102  - Radeon RX 7600

# =============================================================================
# Example Configurations
# =============================================================================

# Example 1: Using LM Studio models with llama.cpp
# MODELS_DIR=/home/user/.lmstudio/models
# MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
# LLAMA_PORT=8000
# GPU_ID=0

# Example 2: Using vLLM with local HuggingFace model
# MODELS_DIR=/home/user/hf-models
# MODEL_NAME=/models/llama-2-7b-chat-hf
# VLLM_PORT=8000
# ROCM_ARCH=gfx1100
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx

# Example 3: Using vLLM with HuggingFace auto-download
# MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
# ROCM_ARCH=gfx1100

# Example 4: Multi-GPU setup
# GPU_ID=0,1
# TENSOR_PARALLEL=2
