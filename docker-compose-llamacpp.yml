# Docker Compose for llama.cpp with ROCm
# Backend: llama.cpp
# Use Case: General-purpose - Official AMD support, production-ready
# Usage: docker-compose -f docker-compose-llamacpp.yml up -d

version: '3.8'

services:
  llama-server:
    image: rocm/llama.cpp:llama.cpp-b5997_rocm6.4.0_ubuntu24.04_server
    container_name: llama-server

    # GPU device access - Manual device mounting (simplest method)
    devices:
      - /dev/kfd
      - /dev/dri

    # Security and performance options
    security_opt:
      - seccomp=unconfined

    # Add video group for GPU access
    group_add:
      - video

    # Enable shared memory for multi-GPU/multi-process
    ipc: host

    # Increase shared memory for large models
    shm_size: 16g

    # Volume mount for GGUF models
    # Create ./models directory and place your .gguf files there
    # Or set MODELS_DIR to your existing model directory
    volumes:
      - ${MODELS_DIR:-./models}:/data:ro

    # Expose llama.cpp server API port
    ports:
      - "${LLAMA_PORT:-8000}:8000"

    # Environment variables
    environment:
      - HIP_VISIBLE_DEVICES=${GPU_ID:-0}

    # Server startup command
    # Set MODEL_PATH to the full path to your .gguf file inside the container
    # Example: /data/llama-2-7b-chat.Q4_K_M.gguf
    #
    # IMPORTANT: --jinja flag is REQUIRED for OpenAI-style tool/function calling
    # Tool calling works with: Llama 3.1/3.3, Mistral Nemo, Qwen 2.5, Hermes, Functionary
    command:
      - "-m"
      - "${MODEL_PATH:-/data/model.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "-ngl"
      - "${GPU_LAYERS:-99}"              # Load all layers to GPU
      - "-c"
      - "${CONTEXT_SIZE:-4096}"          # Context size (adjust based on VRAM)
      - "--n-gpu-layers"
      - "${GPU_LAYERS:-99}"
      - "--parallel"
      - "${PARALLEL_SLOTS:-1}"           # Number of parallel request slots
      - "--jinja"                        # Enable Jinja templates (REQUIRED for tool calling)

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# Environment Variables:
# All variables have defaults - override by setting them before running docker-compose
#
# MODELS_DIR      - Host directory containing your models (default: ./models)
# MODEL_PATH      - Full path to .gguf file inside container (default: /data/model.gguf)
# LLAMA_PORT      - Host port to expose (default: 8000)
# GPU_ID          - GPU device ID (default: 0)
# GPU_LAYERS      - Number of layers to offload to GPU (default: 99)
# CONTEXT_SIZE    - Context window size (default: 4096)
# PARALLEL_SLOTS  - Number of parallel request slots (default: 1)
#                   Set to 2-4 for concurrent request handling
#                   Higher values = more concurrent requests but more VRAM usage
#
# Setup instructions:
#
# Method 1: Using environment variables (recommended)
# 1. Set environment variables:
#    export MODELS_DIR="$HOME/.lmstudio/models"
#    export MODEL_PATH="/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf"
#
# 2. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 2: Using .env file
# 1. Create a .env file in the same directory:
#    cat > .env << EOF
#    MODELS_DIR=/home/user/.lmstudio/models
#    MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
#    LLAMA_PORT=8000
#    GPU_ID=0
#    GPU_LAYERS=99
#    CONTEXT_SIZE=4096
#    EOF
#
# 2. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 3: Inline with docker-compose
#    MODELS_DIR=$HOME/.lmstudio/models \
#    MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf \
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 4: Default setup (for new users)
# 1. Create models directory:
#    mkdir -p models
#
# 2. Download a GGUF model (Q4_K_M recommended):
#    # Example sources:
#    # - https://huggingface.co/TheBloke
#    # - https://huggingface.co/models?library=gguf
#
# 3. Place your model file in ./models/model.gguf
#
# 4. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Common Commands:
# - View logs:
#   docker-compose -f docker-compose-llamacpp.yml logs -f
#
# - Stop the service:
#   docker-compose -f docker-compose-llamacpp.yml down
#
# =============================================================================
# OpenAI-Compatible API Examples
# =============================================================================
#
# llama.cpp server provides OpenAI-compatible endpoints at:
# - POST /v1/chat/completions  (with tool calling support)
# - POST /v1/completions
# - POST /v1/embeddings
# - GET  /v1/models
#
# 1. Basic Chat Completion:
#    curl http://localhost:8000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-3.5-turbo",
#        "messages": [{"role": "user", "content": "Hello!"}]
#      }'
#
# 2. Chat with Tool/Function Calling (Requires --jinja flag and compatible model):
#    curl http://localhost:8000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-3.5-turbo",
#        "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],
#        "tools": [{
#          "type": "function",
#          "function": {
#            "name": "get_weather",
#            "description": "Get the current weather in a location",
#            "parameters": {
#              "type": "object",
#              "properties": {
#                "location": {"type": "string", "description": "City name"},
#                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
#              },
#              "required": ["location"]
#            }
#          }
#        }]
#      }'
#
# 3. Streaming Chat:
#    curl http://localhost:8000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-3.5-turbo",
#        "messages": [{"role": "user", "content": "Count to 10"}],
#        "stream": true
#      }'
#
# 4. JSON Mode:
#    curl http://localhost:8000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-3.5-turbo",
#        "messages": [{"role": "user", "content": "List 3 colors in JSON"}],
#        "response_format": {"type": "json_object"}
#      }'
#
# 5. Text Completion:
#    curl http://localhost:8000/v1/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-3.5-turbo",
#        "prompt": "Once upon a time",
#        "max_tokens": 50
#      }'
#
# 6. Embeddings:
#    curl http://localhost:8000/v1/embeddings \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "text-embedding-ada-002",
#        "input": "Hello world"
#      }'
#
# Python OpenAI SDK Example:
# ---------------------------
# from openai import OpenAI
#
# client = OpenAI(
#     base_url="http://localhost:8000/v1",
#     api_key="not-needed"  # llama.cpp doesn't require API key
# )
#
# # Basic chat
# response = client.chat.completions.create(
#     model="gpt-3.5-turbo",
#     messages=[{"role": "user", "content": "Hello!"}]
# )
# print(response.choices[0].message.content)
#
# # Chat with tools (requires Llama 3.1+, Mistral Nemo, Qwen 2.5, etc.)
# tools = [{
#     "type": "function",
#     "function": {
#         "name": "get_weather",
#         "description": "Get weather in a location",
#         "parameters": {
#             "type": "object",
#             "properties": {
#                 "location": {"type": "string"}
#             },
#             "required": ["location"]
#         }
#     }
# }]
#
# response = client.chat.completions.create(
#     model="gpt-3.5-turbo",
#     messages=[{"role": "user", "content": "What's the weather in Paris?"}],
#     tools=tools
# )
#
# if response.choices[0].message.tool_calls:
#     print("Tool called:", response.choices[0].message.tool_calls[0].function.name)
#
# Tool Calling Requirements:
# --------------------------
# - Server must be started with --jinja flag (already included above)
# - Model must support tool calling. Compatible models:
#   * Llama 3.1 / 3.3 (8B, 70B) - Best overall choice
#   * Mistral Nemo (12B)
#   * Qwen 2.5 (7B, 14B, 32B)
#   * Hermes 2 / 3
#   * Functionary v3.1 / v3.2
#   * Firefunction v2
# - Use Q4_K_M or higher quantization (extreme quantization degrades tool calling)
# - Parallel tool calls disabled by default (most models handle sequential better)
#
# For more details, see: OPENAI_API_GUIDE.md
