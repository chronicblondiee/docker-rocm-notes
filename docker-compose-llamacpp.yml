# Docker Compose for llama.cpp with ROCm
# Backend: llama.cpp
# Use Case: General-purpose - Official AMD support, production-ready
# Usage: docker-compose -f docker-compose-llamacpp.yml up -d

version: '3.8'

services:
  llama-server:
    image: rocm/llama.cpp:llama.cpp-b5997_rocm6.4.0_ubuntu24.04_server
    container_name: llama-server

    # GPU device access - Manual device mounting (simplest method)
    devices:
      - /dev/kfd
      - /dev/dri

    # Security and performance options
    security_opt:
      - seccomp=unconfined

    # Add video group for GPU access
    group_add:
      - video

    # Enable shared memory for multi-GPU/multi-process
    ipc: host

    # Increase shared memory for large models
    shm_size: 16g

    # Volume mount for GGUF models
    # Create ./models directory and place your .gguf files there
    # Or set MODELS_DIR to your existing model directory
    volumes:
      - ${MODELS_DIR:-./models}:/data:ro

    # Expose llama.cpp server API port
    ports:
      - "${LLAMA_PORT:-8000}:8000"

    # Environment variables
    environment:
      - HIP_VISIBLE_DEVICES=${GPU_ID:-0}

    # Server startup command
    # Set MODEL_PATH to the full path to your .gguf file inside the container
    # Example: /data/llama-2-7b-chat.Q4_K_M.gguf
    command:
      - "-m"
      - "${MODEL_PATH:-/data/model.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "-ngl"
      - "${GPU_LAYERS:-99}"              # Load all layers to GPU
      - "-c"
      - "${CONTEXT_SIZE:-4096}"          # Context size (adjust based on VRAM)
      - "--n-gpu-layers"
      - "${GPU_LAYERS:-99}"
      - "--parallel"
      - "${PARALLEL_SLOTS:-1}"           # Number of parallel request slots

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# Environment Variables:
# All variables have defaults - override by setting them before running docker-compose
#
# MODELS_DIR      - Host directory containing your models (default: ./models)
# MODEL_PATH      - Full path to .gguf file inside container (default: /data/model.gguf)
# LLAMA_PORT      - Host port to expose (default: 8000)
# GPU_ID          - GPU device ID (default: 0)
# GPU_LAYERS      - Number of layers to offload to GPU (default: 99)
# CONTEXT_SIZE    - Context window size (default: 4096)
# PARALLEL_SLOTS  - Number of parallel request slots (default: 1)
#                   Set to 2-4 for concurrent request handling
#                   Higher values = more concurrent requests but more VRAM usage
#
# Setup instructions:
#
# Method 1: Using environment variables (recommended)
# 1. Set environment variables:
#    export MODELS_DIR="$HOME/.lmstudio/models"
#    export MODEL_PATH="/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf"
#
# 2. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 2: Using .env file
# 1. Create a .env file in the same directory:
#    cat > .env << EOF
#    MODELS_DIR=/home/user/.lmstudio/models
#    MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf
#    LLAMA_PORT=8000
#    GPU_ID=0
#    GPU_LAYERS=99
#    CONTEXT_SIZE=4096
#    EOF
#
# 2. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 3: Inline with docker-compose
#    MODELS_DIR=$HOME/.lmstudio/models \
#    MODEL_PATH=/data/lmstudio-community/granite-4.0-h-tiny-GGUF/granite-4.0-h-tiny-Q4_K_M.gguf \
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Method 4: Default setup (for new users)
# 1. Create models directory:
#    mkdir -p models
#
# 2. Download a GGUF model (Q4_K_M recommended):
#    # Example sources:
#    # - https://huggingface.co/TheBloke
#    # - https://huggingface.co/models?library=gguf
#
# 3. Place your model file in ./models/model.gguf
#
# 4. Start the service:
#    docker-compose -f docker-compose-llamacpp.yml up -d
#
# Common Commands:
# - View logs:
#   docker-compose -f docker-compose-llamacpp.yml logs -f
#
# - Test the API:
#   curl http://localhost:8000/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"messages": [{"role": "user", "content": "Hello!"}]}'
#
# - Stop the service:
#   docker-compose -f docker-compose-llamacpp.yml down
