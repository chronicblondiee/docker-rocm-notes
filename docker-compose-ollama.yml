# Docker Compose for Ollama with ROCm
# Backend: Ollama
# Use Case: Development - Easiest to use, auto-downloads models
# Usage: docker-compose -f docker-compose-ollama.yml up -d

version: '3.8'

services:
  ollama:
    image: ollama/ollama:rocm
    container_name: ollama-dev

    # GPU device access - Manual device mounting
    devices:
      - /dev/kfd
      - /dev/dri

    # Add video group for GPU access
    group_add:
      - video

    # Persistent storage for models and configuration
    volumes:
      - ${OLLAMA_DATA:-ollama-data}:/root/.ollama

    # Expose Ollama API port
    ports:
      - "${OLLAMA_PORT:-11434}:11434"

    # Environment variables (optional)
    environment:
      - OLLAMA_HOST=0.0.0.0
      - HIP_VISIBLE_DEVICES=${GPU_ID:-0}

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  ollama-data:
    driver: local

# Environment Variables:
# OLLAMA_DATA  - Docker volume or host path for model storage (default: ollama-data named volume)
# OLLAMA_PORT  - Host port to expose (default: 11434)
# GPU_ID       - GPU device ID (default: 0)
#
# Quick start guide:
#
# 1. Start Ollama:
#    docker-compose -f docker-compose-ollama.yml up -d
#
# 2. Pull a model (run from host):
#    docker exec -it ollama-dev ollama pull llama3.2
#
#    Popular models to try:
#    - llama3.2          (Meta's Llama 3.2 - 3B)
#    - llama3.2:1b       (Llama 3.2 - 1B, very fast)
#    - llama3.1:8b       (Llama 3.1 - 8B)
#    - mistral           (Mistral 7B)
#    - mixtral:8x7b      (Mixtral 8x7B MoE)
#    - codellama         (Code Llama 7B)
#    - qwen2.5:7b        (Qwen 2.5 - 7B)
#    - deepseek-r1:7b    (DeepSeek R1 - 7B)
#
# 3. Run a model interactively:
#    docker exec -it ollama-dev ollama run llama3.2
#
# 4. List downloaded models:
#    docker exec -it ollama-dev ollama list
#
# 5. Use the API from host:
#    # Generate response
#    curl http://localhost:11434/api/generate -d '{
#      "model": "llama3.2",
#      "prompt": "Why is the sky blue?",
#      "stream": false
#    }'
#
#    # Chat (conversational)
#    curl http://localhost:11434/api/chat -d '{
#      "model": "llama3.2",
#      "messages": [
#        {"role": "user", "content": "Hello! How are you?"}
#      ],
#      "stream": false
#    }'
#
#    # OpenAI-compatible endpoint
#    curl http://localhost:11434/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "llama3.2",
#        "messages": [{"role": "user", "content": "Hello!"}]
#      }'
#
# 6. View logs:
#    docker-compose -f docker-compose-development.yml logs -f
#
# 7. Remove a model:
#    docker exec -it ollama-dev ollama rm llama3.2
#
# 8. Show model information:
#    docker exec -it ollama-dev ollama show llama3.2
#
# 9. Stop the service:
#    docker-compose -f docker-compose-development.yml down
#
# 10. Stop and remove all data (models):
#     docker-compose -f docker-compose-development.yml down -v
#
# Advanced usage:
#
# Custom model parameters (Modelfile):
# 1. Create a Modelfile on host:
#    echo 'FROM llama3.2
#    PARAMETER temperature 0.8
#    PARAMETER top_p 0.9
#    SYSTEM You are a helpful assistant.' > Modelfile
#
# 2. Copy to container and create custom model:
#    docker cp Modelfile ollama-dev:/tmp/
#    docker exec -it ollama-dev ollama create my-custom-model -f /tmp/Modelfile
#
# 3. Run your custom model:
#    docker exec -it ollama-dev ollama run my-custom-model
#
# API endpoints:
# - GET  /api/tags           - List models
# - POST /api/generate       - Generate response
# - POST /api/chat           - Chat conversation
# - POST /api/pull           - Pull/download model
# - POST /api/push           - Push model to registry
# - POST /api/create         - Create custom model
# - DELETE /api/delete       - Delete model
# - POST /api/show           - Show model info
# - POST /api/embeddings     - Generate embeddings
#
# OpenAI-compatible endpoints:
# - POST /v1/chat/completions
# - POST /v1/completions
# - POST /v1/embeddings
#
# For more info: https://github.com/ollama/ollama/blob/main/docs/api.md
